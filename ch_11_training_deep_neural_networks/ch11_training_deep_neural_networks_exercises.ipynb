{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## *Exercise 8.* Deep Learning on CIFAR10 dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a. Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=1e-2)  # 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "X_test = x_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "early_stop_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
    "run_index = 5  # Increment with each run\n",
    "run_logdir = os.path.join(os.curdir, \"logdir\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "bn_callbacks = [early_stop_cb, checkpoint_cb, tensorboard_cb]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 21s 15ms/step - loss: 12.9480 - accuracy: 0.1876 - val_loss: 2.0913 - val_accuracy: 0.1969\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 2.0087 - accuracy: 0.2420 - val_loss: 1.9820 - val_accuracy: 0.2396\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 18s 15ms/step - loss: 1.9766 - accuracy: 0.2598 - val_loss: 1.9438 - val_accuracy: 0.2648\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.9421 - accuracy: 0.2710 - val_loss: 1.8627 - val_accuracy: 0.3004\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 7611029.0000 - accuracy: 0.1244 - val_loss: 2.3035 - val_accuracy: 0.1017\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 8s 6ms/step - loss: 2.3057 - accuracy: 0.0969 - val_loss: 2.3048 - val_accuracy: 0.1023\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3066 - accuracy: 0.0996 - val_loss: 2.3071 - val_accuracy: 0.0933\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3068 - accuracy: 0.1028 - val_loss: 2.3028 - val_accuracy: 0.1040\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3079 - accuracy: 0.0997 - val_loss: 2.3094 - val_accuracy: 0.1017\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 2.3079 - accuracy: 0.1041 - val_loss: 2.3098 - val_accuracy: 0.1040\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 2.3086 - accuracy: 0.1006 - val_loss: 2.3076 - val_accuracy: 0.0979\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 2.3091 - accuracy: 0.0997 - val_loss: 2.3118 - val_accuracy: 0.1015\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 2.3097 - accuracy: 0.0988 - val_loss: 2.3080 - val_accuracy: 0.0933\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3100 - accuracy: 0.0994 - val_loss: 2.3084 - val_accuracy: 0.0996\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3102 - accuracy: 0.1006 - val_loss: 2.3076 - val_accuracy: 0.0979\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3109 - accuracy: 0.1002 - val_loss: 2.3061 - val_accuracy: 0.1023\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 2.3110 - accuracy: 0.0999 - val_loss: 2.3112 - val_accuracy: 0.0933\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 2.3114 - accuracy: 0.0986 - val_loss: 2.3048 - val_accuracy: 0.1015\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 2.3112 - accuracy: 0.1010 - val_loss: 2.3213 - val_accuracy: 0.0973\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3103 - accuracy: 0.0997 - val_loss: 2.3155 - val_accuracy: 0.1040\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3116 - accuracy: 0.0988 - val_loss: 2.3053 - val_accuracy: 0.1030\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3110 - accuracy: 0.1033 - val_loss: 2.3147 - val_accuracy: 0.0994\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 2.3120 - accuracy: 0.1013 - val_loss: 2.3159 - val_accuracy: 0.1023\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 2.3129 - accuracy: 0.1026 - val_loss: 2.3144 - val_accuracy: 0.1023\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x17024b84e50>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_val, y_val),\n",
    "          callbacks=bn_callbacks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 1.8668 - accuracy: 0.2972\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.8668041229248047, 0.2971999943256378]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"model.h5\")\n",
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 19s 10ms/step - loss: 1.8501 - accuracy: 0.3364 - val_loss: 1.6506 - val_accuracy: 0.4146\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.6753 - accuracy: 0.4036 - val_loss: 1.5814 - val_accuracy: 0.4365\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.6025 - accuracy: 0.4322 - val_loss: 1.5464 - val_accuracy: 0.4507\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.5537 - accuracy: 0.4494 - val_loss: 1.5067 - val_accuracy: 0.4644\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 1.5111 - accuracy: 0.4631 - val_loss: 1.4652 - val_accuracy: 0.4722\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.4738 - accuracy: 0.4755 - val_loss: 1.4795 - val_accuracy: 0.4801\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.4444 - accuracy: 0.4864 - val_loss: 1.4328 - val_accuracy: 0.4891\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.4105 - accuracy: 0.4997 - val_loss: 1.4237 - val_accuracy: 0.4961\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3911 - accuracy: 0.5082 - val_loss: 1.3901 - val_accuracy: 0.5016\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.3659 - accuracy: 0.5175 - val_loss: 1.3767 - val_accuracy: 0.5119\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.3422 - accuracy: 0.5220 - val_loss: 1.3834 - val_accuracy: 0.5109\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.3163 - accuracy: 0.5348 - val_loss: 1.3881 - val_accuracy: 0.5149\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2977 - accuracy: 0.5388 - val_loss: 1.3836 - val_accuracy: 0.5161\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2889 - accuracy: 0.5450 - val_loss: 1.3483 - val_accuracy: 0.5202\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2687 - accuracy: 0.5496 - val_loss: 1.3795 - val_accuracy: 0.5186\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.2477 - accuracy: 0.5589 - val_loss: 1.3917 - val_accuracy: 0.5214\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 1.2249 - accuracy: 0.5672 - val_loss: 1.3469 - val_accuracy: 0.5307\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2207 - accuracy: 0.5679 - val_loss: 1.3680 - val_accuracy: 0.5288\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.2007 - accuracy: 0.5738 - val_loss: 1.3664 - val_accuracy: 0.5245\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1897 - accuracy: 0.5804 - val_loss: 1.3867 - val_accuracy: 0.5246\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.1787 - accuracy: 0.5825 - val_loss: 1.3828 - val_accuracy: 0.5247\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1611 - accuracy: 0.5903 - val_loss: 1.3635 - val_accuracy: 0.5322\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 18s 15ms/step - loss: 1.1482 - accuracy: 0.5942 - val_loss: 1.3528 - val_accuracy: 0.5338\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 1.1335 - accuracy: 0.5992 - val_loss: 1.3677 - val_accuracy: 0.5330\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1231 - accuracy: 0.6033 - val_loss: 1.3744 - val_accuracy: 0.5265\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.1155 - accuracy: 0.6066 - val_loss: 1.3812 - val_accuracy: 0.5236\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.0945 - accuracy: 0.6126 - val_loss: 1.3767 - val_accuracy: 0.5325\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1704a453130>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model = keras.models.Sequential()\n",
    "\n",
    "bn_model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "bn_model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    bn_model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    bn_model.add(keras.layers.BatchNormalization())\n",
    "    bn_model.add(keras.layers.Activation(activation=\"elu\"))\n",
    "bn_model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "bn_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=keras.optimizers.Nadam(learning_rate=5e-4),\n",
    "                 metrics=[\"accuracy\"])\n",
    "\n",
    "bn_early_stop_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "bn_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"bn_model.h5\", save_best_only=True)\n",
    "bn_run_index = 1  # Increment with each run\n",
    "bn_run_logdir = os.path.join(os.curdir, \"logdir\", \"run_bn_{:03d}\".format(bn_run_index))\n",
    "bn_tensorboard_cb = keras.callbacks.TensorBoard(bn_run_logdir)\n",
    "bn_callbacks = [bn_early_stop_cb, bn_checkpoint_cb, bn_tensorboard_cb]\n",
    "\n",
    "bn_model.fit(X_train, y_train,\n",
    "             epochs=100,\n",
    "             validation_data=(X_val, y_val),\n",
    "             callbacks=bn_callbacks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 11ms/step - loss: 1.3469 - accuracy: 0.5245\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.3469046354293823, 0.5245000123977661]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model = keras.models.load_model(\"bn_model.h5\")\n",
    "bn_model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 17s 12ms/step - loss: 1.9227 - accuracy: 0.3083 - val_loss: 1.7873 - val_accuracy: 0.3529\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.7139 - accuracy: 0.3894 - val_loss: 1.7288 - val_accuracy: 0.3872\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6208 - accuracy: 0.4257 - val_loss: 1.6379 - val_accuracy: 0.4249\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.5541 - accuracy: 0.4534 - val_loss: 1.5852 - val_accuracy: 0.4321\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.5056 - accuracy: 0.4732 - val_loss: 1.6143 - val_accuracy: 0.4439\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.4529 - accuracy: 0.4929 - val_loss: 1.5420 - val_accuracy: 0.4675\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.4146 - accuracy: 0.5093 - val_loss: 1.5135 - val_accuracy: 0.4727\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.3751 - accuracy: 0.5216 - val_loss: 1.5080 - val_accuracy: 0.4802\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3431 - accuracy: 0.5325 - val_loss: 1.5075 - val_accuracy: 0.4790\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.3101 - accuracy: 0.5446 - val_loss: 1.5203 - val_accuracy: 0.4822\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.2825 - accuracy: 0.5544 - val_loss: 1.5344 - val_accuracy: 0.4721\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.2507 - accuracy: 0.5663 - val_loss: 1.5237 - val_accuracy: 0.4902\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.2294 - accuracy: 0.5775 - val_loss: 1.5281 - val_accuracy: 0.4892\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.1931 - accuracy: 0.5896 - val_loss: 1.5629 - val_accuracy: 0.4865\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.1754 - accuracy: 0.5944 - val_loss: 1.5157 - val_accuracy: 0.4937\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.1522 - accuracy: 0.6063 - val_loss: 1.5897 - val_accuracy: 0.4863\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.1392 - accuracy: 0.6113 - val_loss: 1.5519 - val_accuracy: 0.4945\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1130 - accuracy: 0.6186 - val_loss: 1.5608 - val_accuracy: 0.4916\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 13s 10ms/step - loss: 1.0972 - accuracy: 0.6261 - val_loss: 1.5588 - val_accuracy: 0.4946\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x170495ca550>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selu_model = keras.models.Sequential()\n",
    "selu_model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "for _ in range(20):\n",
    "    selu_model.add(keras.layers.Dense(100,\n",
    "                                      kernel_initializer=\"lecun_normal\",\n",
    "                                      activation=\"selu\"))\n",
    "selu_model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "selu_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer=optimizer,\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "selu_early_stopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "selu_model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"selu_model.h5\", save_best_only=True)\n",
    "selu_run_index = 1  # increment every time you train the model\n",
    "selu_run_logdir = os.path.join(os.curdir, \"logdir\", \"run_selu_{:03d}\".format(selu_run_index))\n",
    "selu_tensorboard_cb = keras.callbacks.TensorBoard(selu_run_logdir)\n",
    "selu_callbacks = [selu_early_stopping_cb, selu_model_checkpoint_cb, selu_tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_val_scaled = (X_val - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "selu_model.fit(X_train_scaled, y_train,\n",
    "               epochs=100,\n",
    "               validation_data=(X_val_scaled, y_val),\n",
    "               callbacks=selu_callbacks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 1.4851 - accuracy: 0.4842\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.485149621963501, 0.48420000076293945]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selu_model = keras.models.load_model(\"selu_model.h5\")\n",
    "selu_model.evaluate(X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 18s 12ms/step - loss: 1.9249 - accuracy: 0.3135 - val_loss: 1.7442 - val_accuracy: 0.3847\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.6755 - accuracy: 0.4028 - val_loss: 1.6985 - val_accuracy: 0.4096\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5883 - accuracy: 0.4421 - val_loss: 1.6338 - val_accuracy: 0.4258\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 12s 9ms/step - loss: 1.5208 - accuracy: 0.4661 - val_loss: 1.6071 - val_accuracy: 0.4491\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.4664 - accuracy: 0.4839 - val_loss: 1.6467 - val_accuracy: 0.4507\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.4150 - accuracy: 0.5062 - val_loss: 1.5676 - val_accuracy: 0.4699\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 1.3689 - accuracy: 0.5221 - val_loss: 1.5683 - val_accuracy: 0.4662\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 17s 14ms/step - loss: 1.3321 - accuracy: 0.5356 - val_loss: 1.5793 - val_accuracy: 0.4728\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2954 - accuracy: 0.5491 - val_loss: 1.5816 - val_accuracy: 0.4765\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 19s 15ms/step - loss: 1.2613 - accuracy: 0.5636 - val_loss: 1.5240 - val_accuracy: 0.4925\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 1.2214 - accuracy: 0.5796 - val_loss: 1.6237 - val_accuracy: 0.4878\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 12s 10ms/step - loss: 1.1930 - accuracy: 0.5894 - val_loss: 1.7058 - val_accuracy: 0.4785\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 1.1691 - accuracy: 0.5955 - val_loss: 1.6524 - val_accuracy: 0.4853\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1385 - accuracy: 0.6073 - val_loss: 1.6838 - val_accuracy: 0.4950\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.1100 - accuracy: 0.6200 - val_loss: 1.6296 - val_accuracy: 0.4933\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 11s 9ms/step - loss: 1.0864 - accuracy: 0.6286 - val_loss: 1.6605 - val_accuracy: 0.4949\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 10s 8ms/step - loss: 1.0620 - accuracy: 0.6378 - val_loss: 1.6886 - val_accuracy: 0.4925\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 1.0424 - accuracy: 0.6445 - val_loss: 1.7696 - val_accuracy: 0.4864\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.0076 - accuracy: 0.6557 - val_loss: 1.7508 - val_accuracy: 0.4901\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 0.9852 - accuracy: 0.6651 - val_loss: 1.8184 - val_accuracy: 0.4808\n",
      "313/313 [==============================] - 4s 13ms/step - loss: 1.4961 - accuracy: 0.4977\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.4961271286010742, 0.4977000057697296]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_dropout_model = keras.models.Sequential()\n",
    "alpha_dropout_model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    alpha_dropout_model.add(keras.layers.Dense(100,\n",
    "                                               kernel_initializer=\"lecun_normal\",\n",
    "                                               activation=\"selu\"))\n",
    "\n",
    "alpha_dropout_model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "alpha_dropout_model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "alpha_dropout_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                            optimizer=optimizer,\n",
    "                            metrics=[\"accuracy\"])\n",
    "\n",
    "alpha_dropout_early_stopping_cb = keras.callbacks.EarlyStopping(patience=10)\n",
    "alpha_dropout_model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"alpha_dropout_model.h5\", save_best_only=True)\n",
    "alpha_dropout_run_index = 1  # increment every time you train the model\n",
    "alpha_dropout_run_logdir = os.path.join(os.curdir, \"logdir\", \"run_alpha_dropout_{:03d}\".format(alpha_dropout_run_index))\n",
    "alpha_dropout_tensorboard_cb = keras.callbacks.TensorBoard(alpha_dropout_run_logdir)\n",
    "alpha_dropout_callbacks = [alpha_dropout_early_stopping_cb, alpha_dropout_model_checkpoint_cb,\n",
    "                           alpha_dropout_tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_val_scaled = (X_val - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "alpha_dropout_model.fit(X_train_scaled, y_train, epochs=100,\n",
    "                        validation_data=(X_val_scaled, y_val),\n",
    "                        callbacks=alpha_dropout_callbacks)\n",
    "\n",
    "alpha_dropout_model = keras.models.load_model(\"alpha_dropout_model.h5\")\n",
    "alpha_dropout_model.evaluate(X_test_scaled, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for _ in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 2s 6ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 1s 2ms/step\n",
      "313/313 [==============================] - 0s 1ms/step\n",
      "313/313 [==============================] - 1s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.1015"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = mc_dropout_predict_classes(mc_model, X_val_scaled)\n",
    "accuracy = np.mean(y_pred == y_val[:, 0])\n",
    "accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
